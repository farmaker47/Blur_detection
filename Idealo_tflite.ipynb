{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Idealo_tflite.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IWr4MTDDUHvR",
        "outputId": "f9b44c49-7ecd-4660-adb1-f9f009dbf167"
      },
      "source": [
        "!git clone https://github.com/idealo/image-quality-assessment.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'image-quality-assessment'...\n",
            "remote: Enumerating objects: 531, done.\u001b[K\n",
            "remote: Total 531 (delta 0), reused 0 (delta 0), pack-reused 531\u001b[K\n",
            "Receiving objects: 100% (531/531), 79.96 MiB | 4.89 MiB/s, done.\n",
            "Resolving deltas: 100% (214/214), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uHh54MiNUf4d",
        "outputId": "68642d8a-9584-4032-d4c8-d35d68310c45"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import imutils\n",
        "from imutils import paths\n",
        "\n",
        "import argparse\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import os\n",
        "from google.colab.patches import cv2_imshow\n",
        "from google.colab import files\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wm95oQ5RUgVK"
      },
      "source": [
        "# Convert the model\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model('/content/image-quality-assessment/contrib/tf_serving/tfs_models/mobilenet_technical') # path to the SavedModel directory\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save the model.\n",
        "with open('quality_assessment.tflite', 'wb') as f:\n",
        "  f.write(tflite_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zfASh7cPU9rh",
        "outputId": "7b0fb564-73f8-4940-d23f-5fd5389617ac"
      },
      "source": [
        "input_image = np.asarray(tf.keras.preprocessing.image.load_img('/content/image-quality-assessment/readme_figures/images_technical/techncial3.jpg', target_size=(224,224)))\n",
        "input_image = tf.keras.applications.mobilenet.preprocess_input(input_image)\n",
        "input_image = tf.convert_to_tensor(input_image, dtype=tf.float32)\n",
        "\n",
        "input_image = tf.expand_dims(input_image, axis = 0)\n",
        "print(input_image.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 224, 224, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lnEc7RW9VJMt",
        "outputId": "96b14c48-c1af-49f8-b224-caa185979e6a"
      },
      "source": [
        "# Load the TFLite model and allocate tensors.\n",
        "interpreter = tf.lite.Interpreter(model_path=\"/content/quality_assessment.tflite\")\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# Get input and output tensors.\n",
        "input_details = interpreter.get_input_details()\n",
        "print(input_details)\n",
        "output_details = interpreter.get_output_details()\n",
        "print(output_details)\n",
        "\n",
        "# Test the model on random input data.\n",
        "input_shape = input_details[0]['shape']\n",
        "input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)\n",
        "interpreter.set_tensor(input_details[0]['index'], input_image)\n",
        "\n",
        "interpreter.invoke()\n",
        "\n",
        "# The function `get_tensor()` returns a copy of the tensor data.\n",
        "# Use `tensor()` in order to get a pointer to the tensor.\n",
        "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "print(output_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'name': 'input_1:0', 'index': 0, 'shape': array([  1, 224, 224,   3], dtype=int32), 'shape_signature': array([ -1, 224, 224,   3], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
            "[{'name': 'dense_1/Softmax:0', 'index': 130, 'shape': array([ 1, 10], dtype=int32), 'shape_signature': array([-1, 10], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
            "[[0.22768664 0.14084102 0.11603771 0.08682505 0.13035013 0.06748918\n",
            "  0.0558218  0.06213458 0.06899322 0.04382064]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5X4i-6beWm83",
        "outputId": "b79dd7b4-7756-4118-f057-06da0de470bc"
      },
      "source": [
        "def normalize_labels(labels):\n",
        "    labels_np = np.array(labels)\n",
        "    return labels_np / labels_np.sum()\n",
        "\n",
        "def calc_mean_score(score_dist):\n",
        "    score_dist = normalize_labels(score_dist)\n",
        "    return (score_dist*np.arange(1, 11)).sum()\n",
        "\n",
        "sample = calc_mean_score(output_data)\n",
        "\n",
        "print(sample)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.2084428407251835\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2z_vLQq3cnn-"
      },
      "source": [
        "# Test images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0syeNcwcpyR"
      },
      "source": [
        "!unzip /content/blur_images.zip -d /content/blur_images"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95SUExwGdVuZ"
      },
      "source": [
        "data = []\n",
        "general_path = '/content/blur_images/'\n",
        "\n",
        "for filename in os.listdir(general_path):\n",
        "    if filename.endswith(\"jpg\"): \n",
        "        # Your code comes here such as \n",
        "        data.append(general_path + filename)\n",
        "\n",
        "print(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wbjsd7Ees6s"
      },
      "source": [
        "!mkdir quality_processed_images"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYgcQE6edWSI"
      },
      "source": [
        "THRESHOLD = 5.3\n",
        "# loop over the input images\n",
        "for imagePath in data:\n",
        "  # load the image and process it\n",
        "  image = cv2.imread(imagePath)\n",
        "\n",
        "  input_image = np.asarray(tf.keras.preprocessing.image.load_img(imagePath, target_size=(224,224)))\n",
        "  input_image = tf.keras.applications.mobilenet.preprocess_input(input_image)\n",
        "  input_image = tf.convert_to_tensor(input_image, dtype=tf.float32)\n",
        "  input_image = tf.expand_dims(input_image, axis = 0)\n",
        "  \n",
        "  interpreter.set_tensor(input_details[0]['index'], input_image)\n",
        "  interpreter.invoke()\n",
        "  output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "\n",
        "  sample = calc_mean_score(output_data)\n",
        "  \n",
        "  text = \"Not Blurry\"\n",
        "  color = (0,255,0)\n",
        "  # if the focus measure is less than the supplied threshold,\n",
        "  # then the image should be considered \"blurry\"\n",
        "  if sample < THRESHOLD:\n",
        "    text = \"Blurry\"\n",
        "    color = (0,0,255)\n",
        "  # show the image\n",
        "  cv2.putText(image, \"{}: {:.2f}\".format(text, sample), (10, 30),\n",
        "    cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 3)\n",
        "  cv2_imshow(image)\n",
        "  key = cv2.waitKey(0)\n",
        "\n",
        "  name = os.path.split(imagePath)[1]\n",
        "  #print(name)\n",
        "  \n",
        "  #write the processed image to specific folder\n",
        "  cv2.imwrite(('/content/quality_processed_images/'+ name), image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vd9vCcjmgqV_"
      },
      "source": [
        "#zip folder\n",
        "!zip -r /content/quality_processed_images.zip /content/quality_processed_images"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0ABUKm10EM9"
      },
      "source": [
        "## Find the center of image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMB0QWjF0afE"
      },
      "source": [
        "!mkdir cropped_quality_images"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWsYBWfbxO2L"
      },
      "source": [
        "\n",
        "def im_crop_center(img, w, h):\n",
        "    img_width, img_height = img.size\n",
        "    left, right = (img_width - w) / 2, (img_width + w) / 2\n",
        "    top, bottom = (img_height - h) / 2, (img_height + h) / 2\n",
        "    left, top = round(max(0, left)), round(max(0, top))\n",
        "    right, bottom = round(min(img_width - 0, right)), round(min(img_height - 0, bottom))\n",
        "    return img.crop((left, top, right, bottom))\n",
        "\n",
        "\n",
        "for path in data:\n",
        "  print(path)\n",
        "  pil_image = Image.open(path)\n",
        "\n",
        "  open_cv_image = np.array(im_crop_center(pil_image, 640, 480)) \n",
        "  # Convert RGB to BGR \n",
        "  open_cv_image = open_cv_image[:, :, ::-1].copy()\n",
        "  name = os.path.split(path)[1]\n",
        "  cv2.imwrite(('/content/cropped_quality_images/'+ name), open_cv_image)\n",
        "  #cv2_imshow(open_cv_image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eoa8q9JB1QWE"
      },
      "source": [
        "dataCropped = []\n",
        "general_path_cropped = '/content/cropped_quality_images/'\n",
        "\n",
        "for filename in os.listdir(general_path_cropped):\n",
        "    if filename.endswith(\"jpg\"): \n",
        "        # Your code comes here such as \n",
        "        dataCropped.append(general_path_cropped + filename)\n",
        "\n",
        "print(dataCropped)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ck9oiFsE1q-n"
      },
      "source": [
        "!mkdir quality_cropped_processed_images"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvDYRi8h11DK"
      },
      "source": [
        "THRESHOLD = 5.4\n",
        "# loop over the input images\n",
        "for imagePath in dataCropped[:1]:\n",
        "  # load the image and process it\n",
        "  image = cv2.imread(imagePath)\n",
        "\n",
        "  input_image = np.asarray(tf.keras.preprocessing.image.load_img(imagePath, target_size=(224,224)))\n",
        "  input_image = tf.keras.applications.mobilenet.preprocess_input(input_image)\n",
        "  input_image = tf.convert_to_tensor(input_image, dtype=tf.float32)\n",
        "  input_image = tf.expand_dims(input_image, axis = 0)\n",
        "  \n",
        "  interpreter.set_tensor(input_details[0]['index'], input_image)\n",
        "  interpreter.invoke()\n",
        "  output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "\n",
        "  sample = calc_mean_score(output_data)\n",
        "  \n",
        "  text = \"Not Blurry\"\n",
        "  color = (0,255,0)\n",
        "  # if the focus measure is less than the supplied threshold,\n",
        "  # then the image should be considered \"blurry\"\n",
        "  if sample < THRESHOLD:\n",
        "    text = \"Blurry\"\n",
        "    color = (0,0,255)\n",
        "  # show the image\n",
        "  cv2.putText(image, \"{}: {:.2f}\".format(text, sample), (10, 30),\n",
        "    cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 3)\n",
        "  cv2_imshow(image)\n",
        "  key = cv2.waitKey(0)\n",
        "\n",
        "  name = os.path.split(imagePath)[1]\n",
        "  #print(name)\n",
        "  \n",
        "  #write the processed image to specific folder\n",
        "  cv2.imwrite(('/content/quality_cropped_processed_images/'+ name), image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXfGY2xK3p_S"
      },
      "source": [
        "#zip folder\n",
        "!zip -r /content/quality_cropped_processed_images.zip /content/quality_cropped_processed_images"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVRHreH3vn3w"
      },
      "source": [
        "## Use result from cropped to full image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7w7xvImvsY0"
      },
      "source": [
        "!mkdir cropped_quality_images"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6oj4fn2Tvy3q"
      },
      "source": [
        "\n",
        "def im_crop_center(img, w, h):\n",
        "    img_width, img_height = img.size\n",
        "    left, right = (img_width - w) / 2, (img_width + w) / 2\n",
        "    top, bottom = (img_height - h) / 2, (img_height + h) / 2\n",
        "    left, top = round(max(0, left)), round(max(0, top))\n",
        "    right, bottom = round(min(img_width - 0, right)), round(min(img_height - 0, bottom))\n",
        "    return img.crop((left, top, right, bottom))\n",
        "\n",
        "\n",
        "for path in data:\n",
        "  print(path)\n",
        "  pil_image = Image.open(path)\n",
        "\n",
        "  open_cv_image = np.array(im_crop_center(pil_image, 640, 480)) \n",
        "  # Convert RGB to BGR \n",
        "  open_cv_image = open_cv_image[:, :, ::-1].copy()\n",
        "  name = os.path.split(path)[1]\n",
        "  cv2.imwrite(('/content/cropped_quality_images/'+ name), open_cv_image)\n",
        "  #cv2_imshow(open_cv_image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzaZMyxxv3oU"
      },
      "source": [
        "dataCropped = []\n",
        "general_path_cropped = '/content/cropped_quality_images/'\n",
        "\n",
        "for filename in os.listdir(general_path_cropped):\n",
        "    if filename.endswith(\"jpg\"): \n",
        "        # Your code comes here such as \n",
        "        dataCropped.append(general_path_cropped + filename)\n",
        "\n",
        "print(dataCropped)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2mzPxQkv6-0"
      },
      "source": [
        "!mkdir quality_cropped_to_full_processed_images"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRveQbLuv71L"
      },
      "source": [
        "THRESHOLD = 3.7\n",
        "# loop over the input images\n",
        "for imagePath in dataCropped[:10]:\n",
        "  # load the image and process it\n",
        "  name = os.path.split(imagePath)[1]\n",
        "  #image = cv2.imread(imagePath)\n",
        "  image = cv2.imread('/content/blur_images/' + name)\n",
        "\n",
        "  input_image = np.asarray(tf.keras.preprocessing.image.load_img(imagePath, target_size=(224,224)))\n",
        "  input_image = tf.keras.applications.mobilenet.preprocess_input(input_image)\n",
        "  input_image = tf.convert_to_tensor(input_image, dtype=tf.float32)\n",
        "  input_image = tf.expand_dims(input_image, axis = 0)\n",
        "  \n",
        "  interpreter.set_tensor(input_details[0]['index'], input_image)\n",
        "  interpreter.invoke()\n",
        "  output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "\n",
        "  sample = calc_mean_score(output_data)\n",
        "  \n",
        "  text = \"Not Blurry\"\n",
        "  color = (0,255,0)\n",
        "  # if the focus measure is less than the supplied threshold,\n",
        "  # then the image should be considered \"blurry\"\n",
        "  if sample < THRESHOLD:\n",
        "    text = \"Blurry\"\n",
        "    color = (0,0,255)\n",
        "  # show the image\n",
        "  cv2.putText(image, \"{}: {:.2f}\".format(text, sample), (10, 30),\n",
        "    cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 3)\n",
        "  print(name)\n",
        "  cv2_imshow(image)\n",
        "  key = cv2.waitKey(0)\n",
        "  \n",
        "  #write the processed image to specific folder\n",
        "  cv2.imwrite(('/content/quality_cropped_to_full_processed_images/'+ name), image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhZO7o48v-tZ"
      },
      "source": [
        "#zip folder\n",
        "!zip -r /content/quality_cropped_to_full_processed_images.zip /content/quality_cropped_to_full_processed_images"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}